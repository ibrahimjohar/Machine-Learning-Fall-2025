{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0338aee",
   "metadata": {},
   "source": [
    "### decision tree\n",
    "#### is a supervised learning algorithm that is used for both classification and regression, called a tree bec its structured like one with:\n",
    "- root node: the starting pt(the whole dataset)\n",
    "- branches: decision rules based on features\n",
    "- leaf nodes: the final outcomes (class labels or numeric predictions)\n",
    "\n",
    "Think of it like a flowchart:\n",
    "\n",
    "“If Temperature = Hot → check Humidity;\n",
    "if Humidity = High → play = No; else → play = Yes.”\n",
    "\n",
    "Just like KNN “looks” at neighbors to decide, a Decision Tree asks a series of yes/no questions to classify data.\n",
    "\n",
    "### why use a decision tree?\n",
    "Strength --- Explanation\n",
    "Interpretable --- You can literally read the decision logic.\n",
    "Non-linear --- Can handle non-linear relationships between input and output.\n",
    "No need for scaling --- Works fine with raw data, no normalization required.\n",
    "Works with categorical & numeric data --- Unlike KNN, which prefers numeric features.\n",
    "\n",
    "but it also has weaknesses:\n",
    "- can overfit (memorize training data)\n",
    "- its sensitive to small data changes\n",
    "- greedy algorithms, which may be sometimes suboptimal splits\n",
    "\n",
    "### splitting the data\n",
    "the tree keeps dividing/splitting the data at each step\n",
    "\n",
    "each split tries to make the data in each subset as \"pure\" as possible, meaning that each branch should ideally contain one class\n",
    "\n",
    "### entropy (used in ID3)\n",
    "entropy measures disorder or impurity in a dataset, like if all the samples in a node belong to the same class the entropy of it will be equal to 0 (entropy=0); but if they are perfectly mixed then the entropy will be equal to 1 (entropy=1)\n",
    "$$\n",
    "Entropy(S) = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n",
    "where:\n",
    "- \\( S \\) = dataset  \n",
    "- \\( n \\) = number of classes  \n",
    "- \\( p_i \\) = proportion of samples belonging to class \\( i \\)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If there are 9 positive and 5 negative samples:\n",
    "\n",
    "$$\n",
    "p(Yes) = \\frac{9}{14}, \\quad p(No) = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropy(S) = -\\left(\\frac{9}{14}\\log_2\\frac{9}{14} + \\frac{5}{14}\\log_2\\frac{5}{14}\\right) = 0.94\n",
    "$$\n",
    "\n",
    "### information gain (ID3's heart)\n",
    "information gain tells us how much entropy decreases when we split the data by some particular attribute\n",
    "\n",
    "$$\n",
    "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( S \\) = original dataset  \n",
    "- \\( A \\) = attribute used for splitting  \n",
    "- \\( v \\) = possible values of attribute \\( A \\)  \n",
    "- \\( S_v \\) = subset of \\( S \\) for which \\( A = v \\)\n",
    "\n",
    "higher information gain indicates better attribute to split on, hence the **ROOT NODE** is chosen by the **HIGHEST INFORMATION GAIN**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec1979",
   "metadata": {},
   "source": [
    "## STEPS TO IMPLEMENT DECISION TREE\n",
    "1. Start with the root node containing the whole dataset.  \n",
    "2. Use an Attribute Selection Measure (ASM) to choose the best attribute to split the node.  \n",
    "3. Split the dataset into subsets for each possible value of the selected attribute.  \n",
    "4. Create a decision node that contains the chosen attribute.  \n",
    "5. Recursively repeat steps 1–4 for each child node until stopping criteria are met (all samples in a node have the same label or no attributes remain).\n",
    "\n",
    "## ATTRIBUTE SELECTION MEASURE\n",
    "Common ASMs:\n",
    "- **Entropy / Information Gain** (used by ID3): chooses attributes that maximize reduction of entropy.  \n",
    "- **Gini Index** (used by CART): uses impurity measure 1 − Σ(p_i²).  \n",
    "- **Gain Ratio** (used by C4.5): normalizes Information Gain by SplitInfo to avoid bias toward attributes with many values.  \n",
    "- Others: Chi-square, Reduction in Variance (for regression), etc.\n",
    "\n",
    "## Implementing Decision Tree Classifier\n",
    "- From-scratch approach: compute impurity (entropy/Gini), compute split quality (IG/Gini gain), select best attribute, recurse to build a nested structure (often returned as nested `dict`).  \n",
    "- Library approach (scikit-learn): use `DecisionTreeClassifier`, choose `criterion` ('gini' or 'entropy'), tune hyperparameters (depth, leaf size, pruning).\n",
    "\n",
    "## Training model using DT\n",
    "- Fit the tree on training data using chosen ASM and hyperparameters.  \n",
    "- If from-scratch, implement `fit()` by building the nested structure on the training dataset.\n",
    "\n",
    "## Model Testing\n",
    "- Use an unseen test set to compute predictions and evaluate performance metrics (accuracy, precision, recall, F1, confusion matrix).\n",
    "\n",
    "## Model Training Accuracy & Model Testing Accuracy\n",
    "- **Training accuracy**: how well the model fits the training data (can be high for overfitting).  \n",
    "- **Testing accuracy**: measured on held-out data and is the main unbiased indicator of generalization.\n",
    "\n",
    "## Decision Tree Classifier Parameters\n",
    "- **CRITERION**: quality of split; `\"gini\"` (Gini impurity) or `\"entropy\"`/`\"log_loss\"` (information gain / Shannon entropy).  \n",
    "- **SPLITTER**: strategy to choose split at each node: `\"best\"` (best split) or `\"random\"` (random best).  \n",
    "- **MAX_DEPTH**: maximum depth of the tree; `None` lets it grow until leaves are pure or `min_samples_split` threshold.  \n",
    "- **MIN_SAMPLES_SPLIT / MIN_SAMPLES_LEAF**: control minimal samples to split or to place in leaves (used to regularize).  \n",
    "- **CCP_ALPHA**: complexity-cost pruning (post-pruning) parameter in sklearn — higher values produce smaller trees.\n",
    "\n",
    "## Decision Tree using Entropy / Gini — with and without pruning\n",
    "- *Without pruning*: build full tree until leaves are pure (or no attributes remain).  \n",
    "- *With pruning*: restrict using `max_depth`, `min_samples_leaf`, or post-prune with `ccp_alpha` to reduce overfitting.\n",
    "\n",
    "## How to Visualize a Decision Tree\n",
    "- scikit-learn: `plot_tree`, `export_graphviz` + Graphviz, or `sklearn.tree.plot_tree`.  \n",
    "- For from-scratch trees: write a small printer that prints the nested-dict in an indented form or convert to graph description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fa060",
   "metadata": {},
   "source": [
    "### CODE\n",
    "**1. High-level purpose — what these utilities do**\n",
    "\n",
    "- **predict_single(tree, instance)** — given one example (a row), walk the nested-dict tree and return a predicted label.\n",
    "- **predict_df(tree, df)** — call *predict_single* for every row in a DataFrame and return a numpy array of predictions.\n",
    "- **evaluate_predictions(y_true, y_pred)** — compute and print model metrics (accuracy, precision/recall/F1, confusion matrix) using sklearn.metrics.\n",
    "- **_global_default_label** — a fallback label used when the tree encounters missing or unseen attribute values.\n",
    "\n",
    "These utilities let you take the tree that id3(...) produced and actually use it on new data and judge how well it performs.\n",
    "\n",
    "**2. The tree data structure (very important)**\n",
    "\n",
    "Your id3 function returns either:\n",
    "\n",
    "A leaf: a label string like 'Yes' or 'No' (example: \"Yes\"), or\n",
    "\n",
    "A dict with a single key: the attribute name, mapping to a dict of value -> subtree. \n",
    "\n",
    "Example:\n",
    "\n",
    "{\n",
    "  \n",
    "  'Age': {\n",
    "     \n",
    "     'Young': {'Job_Status': {'Employed': 'No', 'Unemployed': 'Yes'}},\n",
    "     \n",
    "     'Senior': 'Yes',\n",
    "     \n",
    "     'Middle': 'Yes'\n",
    "  \n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "So a node is either \"leaf\" or {'Attribute': {value1: subtree1, value2: subtree2, ...}}.\n",
    "\n",
    "**predict_single** relies on that structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(tree_node, instance, default_label=_global_default_label):\n",
    "    #1) leaf case: if the node is not a dict then its a label (terminal)\n",
    "    if not isinstance(tree_node, dict):\n",
    "        return tree_node\n",
    "    \n",
    "    #2) internal node: get the splitting attribute and its respective branches\n",
    "    attr = next(iter(tree_node))\n",
    "    branches = tree_node[attr]\n",
    "    \n",
    "    #3) read the instance's value for that attribute\n",
    "    try:\n",
    "        val = instance[attr]\n",
    "    except Exception:\n",
    "        return default_label\n",
    "    \n",
    "    #4) if the value was seen during training then recurse into that branch\n",
    "    if val in branches:\n",
    "        return predict_single(branches[val], instance, default_label)\n",
    "    \n",
    "    # 5) Unseen value handling: find majority label among reachable leaves,\n",
    "    #    otherwise fall back to global default label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e9b95",
   "metadata": {},
   "source": [
    "Why each part?\n",
    "\n",
    "- **Step 1**: stops recursion — leaf reached.\n",
    "- **Step 2**: obtains which attribute this node uses to split. next(iter(...)) returns the first (and only) key.\n",
    "- **Step 3**: tries to fetch the instance’s attribute value (works whether instance is a pd.Series row or dict).\n",
    "- **Step 4**: normal path if the exact branch exists.\n",
    "- **Step 5**: defensive strategy for unseen categories (common in real data): instead of crashing, we pick the majority label from all leaves under this node or fall back to the overall majority label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8156b",
   "metadata": {},
   "source": [
    "**4 - The helper collect_leaves — what it does**\n",
    "\n",
    "When we hit an unseen value, the code gathers all leaf labels under the current node to pick the majority. That function is just a recursive traversal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99209c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_leaves(subnode):\n",
    "    if not isinstance(subnode, dict):\n",
    "        return [subnode]\n",
    "    \n",
    "    attr2 = next(iter(subnode))\n",
    "    leaves = []\n",
    "    \n",
    "    for b in subnode[attr2].values():\n",
    "        leaves.extend(collect_leaves(b))\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71395c3d",
   "metadata": {},
   "source": [
    "This returns a list like ['Yes','Yes','No','Yes']. \n",
    "\n",
    "Counter(...).most_common(1)[0][0] picks the majority (e.g., 'Yes')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971efde",
   "metadata": {},
   "source": [
    "**5) predict_df and evaluate_predictions**\n",
    "\n",
    "**predict_df** simply loops over rows and accumulates predictions (easy to understand; okay for small/mid datasets).\n",
    "\n",
    "**evaluate_predictions** uses sklearn.metrics to print accuracy, a classification report (precision/recall/F1), and the confusion matrix. These are standard metrics you’ll need in your lab report."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
