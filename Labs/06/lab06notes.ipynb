{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcea0fca",
   "metadata": {},
   "source": [
    "### **SVM**\n",
    "svm is a binary classification algorithm whose goal is to find the best possible boundary(hyperplane) that separates two classes.\n",
    "\n",
    "##### **hyperplane**\n",
    "a hyperplane is the decision boundary\n",
    "- 2D -> hyperplane = a line\n",
    "- 3D -> hyperplane = a plane\n",
    "- n-dim -> hyperplane exists mathematically but cant be visualised\n",
    "\n",
    "#### **support vectors**\n",
    "these are the data pts that are closest to the hyperplane\n",
    "\n",
    "they define:\n",
    "- the boundary\n",
    "- determine where the hyperplane sits\n",
    "- if they move even slightly -> the hyperplane moves\n",
    "\n",
    "hence these are rlly critical\n",
    "\n",
    "#### **what does SVM optimize?**\n",
    "svm wants the hyperplane w/ maximum margin, margin is the dist btw support vectors and hyperplane\n",
    "- **why?**\n",
    "    - a larger margin = more robust to future data\n",
    "    - a smaller margin = sensitive, overfitting\n",
    "\n",
    "A large margin helps because:\n",
    "- it reduces the model’s sensitivity to tiny shifts or noise in the data.\n",
    "- it lowers overfitting — the classifier isn't “hugging” specific points.\n",
    "- it generalizes better to new, unseen samples, because the boundary is placed in a more stable region of the feature space.\n",
    "\n",
    "*\"SVM prefers the hyperplane with the largest possible margin because it leads to a more stable, robust, and generalizable classifier.\"*\n",
    "\n",
    "#### **linearly separable vs non-linear data**\n",
    "sometimes data cannot be separated w/ a straight line\n",
    "##### **how does SVM handle this?**\n",
    "##### using kernels (kernel trick)\n",
    "- map data from low dimension -> higher dimension\n",
    "- in higher dimension the data becomes separable\n",
    "- svm then finds a linear hyperplane there\n",
    "- the model still behaves non-linear in original space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3879ff",
   "metadata": {},
   "source": [
    "#### **original (x,y)**\n",
    "#### **new feature z = x² + y²**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd312847",
   "metadata": {},
   "source": [
    "##### **1. linear kernel**\n",
    "- decision boundary is a straight line\n",
    "- works when data is linearly separable\n",
    "- fastest, simple\n",
    "- rarely used in real-world, if data is complex\n",
    "\n",
    "##### **2. polynomial kernel**\n",
    "- expands features to polynomials\n",
    "- can capture curved boundaries\n",
    "- but computationally expensive\n",
    "\n",
    "##### **3. rbf/gaussian kernel**\n",
    "- projects points into infinite-dimensional feature space\n",
    "- great for irregular boundaries\n",
    "- often highest accuracy\n",
    "- but can overfit if gamma is too high\n",
    "\n",
    "##### **hard margin vs soft margin**\n",
    "##### **hard margin**\n",
    "- no misclassification is allowed\n",
    "- only works when is perfectly separable\n",
    "- very sensitive to outliers -> rarely used\n",
    "##### **soft margin**\n",
    "- allows some points inside the margin **using slack variables *ξi***\n",
    "$$\n",
    "\\min_{\\mathbf{w},\\,b,\\,\\boldsymbol{\\xi}} \\quad \\frac{1}{2}\\|\\mathbf{w}\\|^{2} + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to:} \\quad \n",
    "y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i,\n",
    "\\qquad \n",
    "\\xi_i \\ge 0,\\quad i = 1,2,\\dots,n\n",
    "$$\n",
    "\n",
    "##### **hyperparameters in SVM**\n",
    "##### -> **1. *C* parameter**\n",
    "controls tradeoff between:\n",
    "- maximising margin\n",
    "- minimising misclassification\n",
    "- **HIGH C** -> Less Margin (marginal dist decreases), fewer misclassified points(errors decrease) -> risk of overfitting\n",
    "- **LOW C** -> Bigger Margin (marginal dist increases), more misclassification allowed (errors increase) -> generalizes better - might underfit if c is too small\n",
    "\n",
    "\n",
    "**using slack variables *ξi***\n",
    "- if *correctly classified*: $$slack = 0$$\n",
    "- if *correctly classified but in margin*: $$0 < slack < 1$$\n",
    "- if *misclassified*: $$slack > 1$$\n",
    "\n",
    "##### -> **2. *Gamma* parameter (RBF only)**\n",
    "controls the influence of each training point\n",
    "- **HIGH *GAMMA*** ->  fewer data points will influencce the decision boundary, hence the boundary becomes non-linear leading to overfiting\n",
    "\n",
    "- **LOW *GAMMA*** -> more data points will influence the decision boundary, hence the boundary is more generic/smoother -> underfits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92707945",
   "metadata": {},
   "source": [
    "#### **implementation flow - code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a094f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "modelSVC = svc.fit(x_train, y_train)\n",
    "prediction = modelSVC.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
