{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748556f3",
   "metadata": {},
   "source": [
    "#### **logistic regression**\n",
    "\n",
    "types of logistic regression\n",
    "- ordinal logistic reg\n",
    "- binary logistic reg\n",
    "- multinomial logistic reg\n",
    "\n",
    "##### **binary logistic regression**\n",
    "used to predict the probability of a binary outcome, like yes or no, true or false, 0 or 1. \n",
    "\n",
    "it could be used to predict whether a customer will churn or not, whether a patient has a disease or not, or whether a loan will be repaid or not\n",
    "\n",
    "##### **multinomial logistic regression**\n",
    "used to predict the probability of one of three or more possible outcomes, such as the type of product a customer will buy, the rating customer will give a product, or the the political party a person will vote for.\n",
    "\n",
    "##### **ordinal logistic regression**\n",
    "used to predict the probability of an outcome that falls into a predetermined order, like the level of customer satisfaction, the severity of a disease, or the stage of cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24b06a",
   "metadata": {},
   "source": [
    "##### **sigmoid function**\n",
    "- mathematical function used to map the predicted values to probabilities\n",
    "- maps any real value into another value within a range of 0 and 1\n",
    "- value of logistic regression must be between 0 and 1, which cant go beyond this limit, hence forming a \"S\" like-curve, this curve is called the sigmoid function or the logistic function\n",
    "- in logistic regression, the concept of threshold value, defines the probability of either 0 or 1. such as values above the threshold value tends to 1, and value below the threshold tends to 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f9a97",
   "metadata": {},
   "source": [
    "##### **cost function**\n",
    "represents optimization objective that is we have to create a cost function and minimize it so that we can develop an accurate model with minimum error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42455197",
   "metadata": {},
   "source": [
    "##### **gradient descent**\n",
    "how do we reduce the cost? (objective in a way)\n",
    "\n",
    "this can be done using _gradient descent_\n",
    "\n",
    "the main goal of GD is to minimize the cost value which is _J(Î¸)_. to minimize the cost function we need to run the gradient descent function on each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a50b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "modelLR = LR.fit(X_train, y_train)\n",
    "prediction = modelLR.predict(X_test)\n",
    "\n",
    "#evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#training accuracy\n",
    "trainAcc = LR.score(X_train, y_train)\n",
    "\n",
    "#testing accuracy\n",
    "testAcc = accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to explore slope and coefficients\n",
    "\n",
    "modelLR.coef_\n",
    "\n",
    "modelLR.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbb39d",
   "metadata": {},
   "source": [
    "##### **multinomial logistic regression**\n",
    "also known as softmax regression/max entropy classifier. its an extension of binary logistic regression, designed to handle problem with more than 2 categories. its a classification algo thats used when the dependent variable is categorical with more than two levels\n",
    "\n",
    "in multinomial logistic regression, the model predicts the probability of each category and assigns the observation to the category with the highest probability, the output is a probability distribution over multiple classes and the probabilities sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLR = LogisticRegression(multi_class='multinomial')\n",
    "\n",
    "modelMLR = MLR.fit(X_train, y_train)\n",
    "\n",
    "prediction = modelMLR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc759bc",
   "metadata": {},
   "source": [
    "**Binary Classification (Two Classes):** \n",
    "\n",
    "If there are only two unique classes in the target variable, logistic regression defaults to the binary classification strategy ('ovr' or 'ovr_balanced' depending on the value of the class_weight parameter). \n",
    "\n",
    "**Multiclass Classification (More than Two Classes):** \n",
    "\n",
    "If there are more than two unique classes, logistic regression automatically switches to the \"multinomial\" strategy. In this strategy, the model is trained to handle multiple classes simultaneously, and it optimizes a single objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver in logistic regression\n",
    "\n",
    "model1 = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077b5177",
   "metadata": {},
   "source": [
    "##### **solver in logistic regression**\n",
    "solver parameter also plays a role in determining the behavior\n",
    "- If **solver='liblinear'**, it is designed for _binary classification_ and uses the **_'ovr'_ strategy** for _multiclass problems_\n",
    "-  If solver is set to other algorithms like **'lbfgs'**, **'newton-cg'**, or **'sag'**, logistic regression'\n",
    "\n",
    "choice of solver depends on various factors like the size of the dataset, num of features and regularization requirements\n",
    "- For small to medium-sized datasets, '**liblinear**' is often a good choice. \n",
    "- For high-dimensional datasets, **'lbfgs'** or **'newton-cg'** might be more efficient.\n",
    "- For large datasets, especially when the number of samples is much larger than the number of features, **'sag'** or **'saga'** could be beneficial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6f5b5",
   "metadata": {},
   "source": [
    "##### penalty in logistic regression\n",
    "penalty parameter is used to specify the type of regularization applied to the model.\n",
    "\n",
    "**regularization** is a technique used to prevent overfitting by adding a penalty term to the loss function\n",
    "\n",
    "the penalty parameter has two main option **'L1'** and **'L2'**, each corr to a diff type of regularization\n",
    "\n",
    "#### **L1 Regularization('l1')**\n",
    "- known as **Lasso Regularization**\n",
    "- adds the absolute values of the coefficients as a penalty term to the loss function\n",
    "- encourages sparsity in the feature coefficients, leading to some coefficients being exactly _zero_\n",
    "- useful for feature selection if we suspect that many features are irrelevant\n",
    "\n",
    "#### **L2 Regularization('l2')**\n",
    "- known as **Ridge Regularization**\n",
    "- adds the squared values of the coefficients as a penalty term to the loss function\n",
    "- encourages small, but non-zero, coefficients for all features\n",
    "- generally used when all features are expected to contribute to the prediction\n",
    "\n",
    "#### **Elastic Net Regularization('elasticnet')**\n",
    "- combination of **L1 & L2**\n",
    "- l1-ratio parameter controls the balance between L1 & L2 regularization\n",
    "- useful when there are many features, and some of them are expected to be irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f42d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#L1 reg\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "#L2 reg\n",
    "model = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "\n",
    "#elastic net reg\n",
    "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ed30b",
   "metadata": {},
   "source": [
    "#### **perceptron**\n",
    "is a fundamental building block of artificial neural networks and serves as a basic computational unit. perceptron is a type of linear classifier that takes a set of binary inputs and produces a binary output, simple mathematical model inspired by the way biological neurons work\n",
    "\n",
    "##### **key components related to perceptron:**\n",
    "- **inputs and weights**\n",
    "    - perceptron receives multiple binary inputs (0 or 1)\n",
    "    - eahc input is associated with a weight, that will determine the influence of that input on the perceptron's output\n",
    "- **weighted sum**\n",
    "    - weighted sum of inputs and weights is calculated\n",
    "    - perceptron performs a weighted sum: $$sum= âˆ‘^n_{i=1} = input_i * weight_i$$\n",
    "- **activation function**\n",
    "    - weighted sum is passed thru an activation function\n",
    "    - result is typically compared to a threshold\n",
    "    - if reseult > threshold, the perceptron outputs 1 OTHERWISE it outputs 0\n",
    "- **thresholding**\n",
    "    - activation function acts as a thresholding function\n",
    "\n",
    "$$\n",
    "\\text{output} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\sum_{i=1}^{n} \\text{input}_i \\times \\text{weight}_i \\ge \\text{threshold} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3820d4",
   "metadata": {},
   "source": [
    "Perceptron have limitations, particularly their inability to learn more complex patterns and relationships. However, they form the basis for more advanced neural network architectures. Multilayer perceptron (MLPs) and other neural network structures were later developed to address the limitations of single-layer perceptron, allowing for the learning of more complex functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e884f",
   "metadata": {},
   "source": [
    "##### **neural network**\n",
    "a computational model inspired by the way biological neural networks in the human brain work. it consists of interconnected nodes, often referred to as neurons or artificial neurons, organised into layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a3990",
   "metadata": {},
   "source": [
    "##### **components of neural network**\n",
    "- **neurons (nodes)**\n",
    "    - neurons are the basic computational unit in a neural network\n",
    "    - each neuron receives one or more inputs, performs a computation and produces an output\n",
    "- **layers**\n",
    "    - neurons are organised into layers, 3 main types of layers:\n",
    "        - **input layer**: receives the initial input data\n",
    "        \n",
    "        - **hidden layers**: intermediate layers between the input and output layers, DNN have multiple hidden layers\n",
    "\n",
    "        - **output layer**: produces the final output or prediction\n",
    "- **weights and connections**\n",
    "    - connections between neurons are associated with weights\n",
    "    - weights determine the strength of the connection between neurons\n",
    "    - learning in a neural network involves adjusting these weights based on training data\n",
    "- **activation function**\n",
    "    - each neuron typically applies an activation function to its weighted sum of inputs\n",
    "    - common activation functions include **sigmoid**, **hyperbolic tangent(*tanh*)**, and **rectified linear unit (ReLu)**\n",
    "- **feedforward and backpropagation**\n",
    "    - in a **feedforward neural network**, information flows from the input layer thru the hidden layers to the output layer\n",
    "\n",
    "    - **backpropagation** is a training algorithm used to adjust weights based on the error between predicted and actual outputs\n",
    "- **training and learning**\n",
    "    - neural networks learn from data by adjusting their weights during the training process\n",
    "    - training involves presenting input data with known outputs and updating weights to minimize prediction errors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2ac3d",
   "metadata": {},
   "source": [
    "#### **multi-layer perceptron (MLP)**\n",
    "refers to a type of neural network architecture. an MLP is a class of feedforward ANN characterised by having multiple layers of nodes (neurons), including an input layer, one or more hidden layers, and an output layer. each layer being fully connected to the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25af7b",
   "metadata": {},
   "source": [
    "**input layer with one hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a683914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add an input layer with 20 neurons (input features)\n",
    "model.add(Dense(units=20, input_dim=20, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed769d7",
   "metadata": {},
   "source": [
    "**2nd hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a hidden layer with 10 neurons\n",
    "model.add(Dense(units=10, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24688655",
   "metadata": {},
   "source": [
    "**output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add an output layer with 1 neuron (binary classification)\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a748ea7",
   "metadata": {},
   "source": [
    "**compile MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf132bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1115c8e",
   "metadata": {},
   "source": [
    "**model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfe739",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9bcb6",
   "metadata": {},
   "source": [
    "#### **what are activation functions?**\n",
    "decides whether a neuron should be \"activated\" or not, that is whether it should pass its signal to the next layer\n",
    "- without activation functions, a NN would just be a linear model, no matter how many layers it had\n",
    "- activation functions introduce non-linearity, allowing models to learn complex patterns\n",
    "\n",
    "\n",
    "1. **Sigmoid** \n",
    "\n",
    "    â€¢ For very large or very small inputs, derivative â†’ 0 â†’ vanishing gradient. \n",
    "\n",
    "    â€¢ Example: If z = 10 â†’ gradient â‰ˆ 0 â†’ learning slows down. \n",
    "2. **Tanh**\n",
    "\n",
    "    â€¢ Also suffers from vanishing gradient at large |z|. \n",
    "    \n",
    "    â€¢ Slightly better than sigmoid because output is zero-centered. \n",
    "3. **ReLU** \n",
    "\n",
    "    â€¢ Positive side â†’ great, gradient = 1 \n",
    "    \n",
    "    â€¢ Negative side â†’ gradient = 0 â†’ neuron can die (never activate again). \n",
    "    \n",
    "    â€¢ Fix: Leaky ReLU or Parametric ReLU \n",
    "4. **Softmax** \n",
    "\n",
    "    â€¢ Great for classification outputs, but cannot be used in hidden layers. \n",
    "    \n",
    "    â€¢ Sensitive to very large input values (overflow) â†’ can use log-softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982aea35",
   "metadata": {},
   "source": [
    "##### **choosing activation functions**\n",
    "- hidden layers: usually ReLU or Leaky ReLu (fast, non-linear)\n",
    "- output layer: depends on the task\n",
    "    - regression -> Linear Activation (no functions)\n",
    "    - binary classification -> Sigmoid\n",
    "    - multi-class classification -> Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccf2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLu\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation=None, input_shape=(20,)),\n",
    "    LeakyReLu(alpha=0.01),\n",
    "    Dense(32, activation=None),\n",
    "    LeakyReLu(alpha=0.01),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295409c6",
   "metadata": {},
   "source": [
    "##### **choosing loss function**\n",
    "- regression -> **MSE** (penalises large errors)\n",
    "\n",
    "- regression -> **MAE**\n",
    "\n",
    "- classification (BINARY) -> **Binary Cross-Entropy** (sigmoid output)\n",
    "\n",
    "- classification (MULTI_CLASS) -> **Categorical Cross-Entropy** (Softmax output)\n",
    "\n",
    "- classification (MULTI-CLASS, INTEGER) -> **Sparse Categorical Cross-Entropy** (integer labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60184dab",
   "metadata": {},
   "source": [
    "#### **gradient descent**\n",
    "goal of gradient descent is to minimize the loss function (e.g., MSE cross-entropy) by adjusting the modelâ€™s parameters (weights, biases). At each step, we move in the opposite direction of the gradient (because it points toward the steepest increase). \n",
    "\n",
    "$$ğœƒ_{new} = ğœƒ_{old} âˆ’ ğœ‚âˆ‡_{ğœƒ}ğ½(ğœƒ)$$\n",
    "\n",
    "ğœƒ = model parameters (weights)\n",
    "\n",
    "ğœ‚ = learning rate \n",
    "\n",
    "âˆ‡ğœƒ ğ½(ğœƒ) = gradient of the loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cf1d3",
   "metadata": {},
   "source": [
    "##### **types of gradient descents**\n",
    "difference lies in how much data we use to compute the gradient each step\n",
    "\n",
    "- **batch gradient descent**\n",
    "    - Uses the entire training dataset to compute the gradient before taking one update step. \n",
    "    - One iteration = one forward + backward pass over all samples.\n",
    "    - **ğœƒ = ğœƒ âˆ’ ğœ‚â€‰âˆ‡ğœƒğ½(ğœƒ; all data)** \n",
    "- **stochastic gradient descent (SGD)**\n",
    "    - updates parameters after every single sample\n",
    "    - **ğœƒ = ğœƒ âˆ’ ğœ‚â€‰âˆ‡ğœƒğ½(ğœƒ; x_i, y_i)** \n",
    "- **mini-batch gradient descent**\n",
    "    - uses a small batch of samples (like 32, 64) to compute the gradient at each step\n",
    "    - most commonly used in practice\n",
    "    - **ğœƒ = ğœƒ âˆ’ ğœ‚â€‰âˆ‡ğœƒğ½(ğœƒ; batch)**\n",
    "    - model.fit(X, y, epochs=50, batch_size=32)\n",
    "        - **batch_size = 32** -> _mini-batch GD_\n",
    "        - **batch_size = len(X)** -> _batch GD_\n",
    "        - **batch_size = 1** -> _stochastic GD_\n",
    "    - model.fit(X, y, validation_split=0.2, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295a709",
   "metadata": {},
   "source": [
    "#### **regularization**\n",
    "a technique to prevent overfitting in ML models\n",
    "- overfitting happens when a model learns not just the true pattern, but also the noise in the training data\n",
    "- regularization adds a penalty to the model's complexity to encourage simpler models to generalize better to new data\n",
    "\n",
    "example:\n",
    "\n",
    "neural network is trying to predict an output from 20 features. \n",
    "- Without regularization â†’ the network might assign huge weights to some features just to perfectly fit your training data. \n",
    "- Regularization â†’ encourages the network to keep weights smaller and balanced, so it doesnâ€™t overreact to small noise in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529a3d5",
   "metadata": {},
   "source": [
    "**types of regularization**\n",
    "- **_L1 Regularization_ (_LASSO_)**\n",
    "    - Adds the absolute value of weights to the loss\n",
    "    $$ğ¿ğ‘œğ‘ ğ‘ _{ğ‘›ğ‘’ğ‘¤} = ğ¿ğ‘œğ‘ ğ‘ _{ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™} + ğœ†âˆ‘ âˆ£ ğ‘¤_ğ‘– âˆ£$$\n",
    "    - Encourages sparsity â†’ some weights become exactly 0, effectively removing irrelevant features.\n",
    "    - Useful if you suspect some of your 20 features are not important.\n",
    "- **_L2 Regularization_ (_RIDGE_)**\n",
    "    - Adds the squared value of weights to the loss\n",
    "    $$ğ¿ğ‘œğ‘ ğ‘ _{ğ‘›ğ‘’ğ‘¤} = ğ¿ğ‘œğ‘ ğ‘ _{ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™} + ğœ†âˆ‘ âˆ£ {ğ‘¤_ğ‘–}^2 âˆ£$$\n",
    "    - Encourages small weights but rarely zero.\n",
    "    - Makes the model less sensitive to any single feature, reducing overfitting. \n",
    "- **_Dropout_**\n",
    "    -  a method where, during training, we randomly â€œdropâ€ (ignore) a fraction of neurons in a layer\n",
    "    - Dropped neurons do not participate in forward pass or backpropagation for that training step\n",
    "    - Each training step uses a different random subset of neurons.\n",
    "    - During testing/inference, all neurons are used, but their outputs are scaled to match the effect of dropout.\n",
    "    - Purpose:\n",
    "        - Prevent the network from relying too heavily on specific neurons â€” encourages it to learn redundant, robust representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fabc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    Dropout(0.2),   #drop 20% of neurons randomly\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)    #regression output\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386badc1",
   "metadata": {},
   "source": [
    "**MLP for Binary Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dca1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "#creating MLP model\n",
    "model = Sequential()\n",
    "\n",
    "#adding an input layer with 20 neurons (input features)\n",
    "model.add(Dense(units=20, input_dim=20, activation='relu'))\n",
    "\n",
    "#adding hidden layer with 10 neurons\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "\n",
    "#adding an ouput layer with 1 neuron (binary classification)\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test))\n",
    "\n",
    "#make predictions of the test set\n",
    "y_pred = model.predit(x_test)\n",
    "y_pred_binary = np.round(y_pred)\n",
    "\n",
    "#evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a779e7d",
   "metadata": {},
   "source": [
    "For Training a binary classification model , the input dim should be equal to the number of featurtes, activation function in input and hidden can be choosen as Relu or any other. But in Output Layer we have to choose Sigmoid for Binary Classification. The loss parameter in compile should be Binary Cross entropy for Binary Classification. The number of nodes in Output Layer should be One for Binary Classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ef662",
   "metadata": {},
   "source": [
    "**MLP for Multi Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23fb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating MLP model\n",
    "model = Sequential()\n",
    "\n",
    "#adding an input layer with 20 neurons (input features)\n",
    "model.add(Dense(units=20, input_dim=4, activation='relu'))\n",
    "\n",
    "#adding hidden layer with 10 neurons\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "\n",
    "#adding an ouput layer with total unique attribute in target variable (multi-classification)\n",
    "model.add(Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test))\n",
    "\n",
    "#make predictions of the test set\n",
    "y_pred = model.predit(x_test)\n",
    "y_pred_binary = np.round(y_pred)\n",
    "\n",
    "#evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47699703",
   "metadata": {},
   "source": [
    "For Training a Multi classification model , the input dim should be equal to the number of features, activation function in input and hidden can be choosen as Relu or any other. But in Output Layer we have to choose Softmax for Multi Classification. The loss parameter in compile should be Categorical Cross entropy for Multi Classification. The number of nodes in Output layer should be equal to the unique records found in Class/Target Variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6dec55",
   "metadata": {},
   "source": [
    "**MLP for Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating MLP model\n",
    "model = Sequential()\n",
    "\n",
    "#adding an input layer with 20 neurons (input features)\n",
    "model.add(Dense(units=20, input_dim=4, activation='relu'))\n",
    "\n",
    "#adding hidden layer with 10 neurons\n",
    "model.add(Dense(units=10, activation='relu'))\n",
    "\n",
    "#adding an ouput layer with 1 neuron (regression)\n",
    "model.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test))\n",
    "\n",
    "#make predictions of the test set\n",
    "y_pred = model.predit(x_test)\n",
    "y_pred_binary = np.round(y_pred)\n",
    "\n",
    "#evaluate accuracy\n",
    "mse = mean_squared_error(y_test, y_pred_binary)\n",
    "print(\"mean sq error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a1cba",
   "metadata": {},
   "source": [
    "For Training a Regression model , the input dim should be equal to the number of features, activation function in input and hidden can be choosen as Relu or any other. But in Output Layer we have to choose Linear for Regression. The loss parameter in compile should be Mean Square Error for Regression. The number of nodes in Output layer should be equal to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
